{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084cd65b",
   "metadata": {},
   "source": [
    "First lets install the correct packages for GPT3.  We are already in the conda environment from jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da380a",
   "metadata": {},
   "source": [
    "First lets install pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed85d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf283b7",
   "metadata": {},
   "source": [
    "Now lets install HuggingFace.  It makes using popular Tranformers MUCH easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf424c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c huggingface transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aba525",
   "metadata": {},
   "source": [
    "Lets import the needed packages now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bfee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c37a4",
   "metadata": {},
   "source": [
    "Now lets get the model.  We can either run the 1.3 billion paramater model or the 2.7 billion parameter model. Lets do the 2.7B model, which is \"EleutherAI/gpt-neo-2.7B\".  The 1.3B model is \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d00f01",
   "metadata": {},
   "source": [
    "This model can be ran on a GPU, but does not have to be. The 2.7B model takes slightly less than 13 GB of Vram.  The 1.3B model takes slighly less than 7.5GB of Vram.  The model will be placed on the GPU if there is one and if there is enough Vram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c2bc2",
   "metadata": {},
   "source": [
    "Lets install pynvml to take a look at how much VRAM we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bde9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in /home/blake/anaconda3/envs/jupyter_gpt/lib/python3.7/site-packages (8.0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf1002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a GPU with 19.6395GB of free VRAM\n"
     ]
    }
   ],
   "source": [
    "free_vram = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    from pynvml import *\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    free_vram = info.free/1048576000\n",
    "    print(\"There is a GPU with \" + str(free_vram) + \"GB of free VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9834d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"EleutherAI/gpt-neo-2.7B\" and free_vram>13.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "elif model_name == \"EleutherAI/gpt-neo-1.3B\" and free_vram>7.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf440c",
   "metadata": {},
   "source": [
    "Now we need to load the tokenizer to prepare the input for GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd79a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b4cbf",
   "metadata": {},
   "source": [
    "We are almost done. At this point we need to decide what prompt we need to decide what prompt we want the model to continue, as well a how long we want the generated output to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = str(input(\"Please enter a prompt: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3361b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = int(input(\"How long should the generated output be? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af23dde",
   "metadata": {},
   "source": [
    "We now need to tokenize the input prompt to prepare it for use with the model.  If we are using a GPU we will put it on the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "if use_cuda:\n",
    "    input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4298458",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0e235e7d327db10aaf4d0d149da6bfca091eadef0e95412c42ab20075da621c"
  },
  "kernelspec": {
   "display_name": "gpt_vid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
